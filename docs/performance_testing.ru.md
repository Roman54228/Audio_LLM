# üìà –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

**–ü—Ä–µ–¥–∏—Å–ª–æ–≤–∏–µ**: –ú—ã –æ—Ü–µ–Ω–∏–≤–∞–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–æ–ª—å–∫–æ –∞–∫—É—Å—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∞ —è–≤–ª—è–µ—Ç—Å—è –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ—Å—É—Ä—Å–æ–µ–º–∫–æ–π —á–∞—Å—Ç—å—é –≤—Å–µ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞.

–î–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—Ç–æ–∫–æ–≤–æ–π –∞–∫—É—Å—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏ –∏ –∏—Å–ø–æ–ª—å–∑—É—é—â–∏—Ö –µ—ë —Å–µ—Ä–≤–∏—Å–æ–≤ –º—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã:

- [`Microsoft ONNX Runtime`](https://github.com/microsoft/onnxruntime/) + [`Nvidia Triton Inference Server`](https://github.com/triton-inference-server/server) = üèÉüí®
–ü—Ä–æ—Å—Ç–∞—è –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏, –Ω–æ –Ω–µ —Å–∞–º–∞—è –±—ã—Å—Ç—Ä–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –±–µ–π–∑–ª–∞–π–Ω–∞.
- [`NVIDIA TensorRT`](https://github.com/NVIDIA/TensorRT/) + [`NVIDIA Triton inference server`](https://github.com/triton-inference-server/server) = ‚ö°Ô∏èüèÉüí®üí®
–ì–æ—Ä–∞–∑–¥–æ –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ (–≤ 2-3 —Ä–∞–∑–∞), –Ω–æ —Ç—Ä–µ–±—É–µ—Ç –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ—Ä–º–∞—Ç TensorRT.

## –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞ —Å –º–æ–¥–µ–ª—å—é

–ü–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤ `ONNX` –∏–ª–∏ `TensorRT` –∏ –∑–∞–ø—É—â–µ–Ω–∞ –≤ Triton Inference Server. –°–º–æ—Ç—Ä–∏—Ç–µ —Ä–∞–∑–¥–µ–ª [Triton Inference Server](docs/triton_inference_server.ru.md) —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏.

## –ó–∞–º–µ—Ä—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### trtexec

–î–ª—è –∑–∞–ø—É—Å–∫–∞ —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–∫—É—Å—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ `TensorRT` –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å cli —É—Ç–∏–ª–∏—Ç—É [**`trtexec`**](https://docs.nvidia.com/deeplearning/tensorrt/latest/reference/command-line-programs.html).

**–ö–ª—é—á–µ–≤—ã–µ —Ñ–ª–∞–≥–∏:**

- `--warmUp`: –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å "–ø—Ä–æ–≥—Ä–µ–≤–∞" –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º –∏–∑–º–µ—Ä–µ–Ω–∏–π.
- `--duration=`: –ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∏–∑–º–µ—Ä–µ–Ω–∏—è –≤ —Å–µ–∫—É–Ω–¥–∞—Ö.
- `--iterations`: –í –∫–∞—á–µ—Å—Ç–≤–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã, –∑–∞–ø—É—Å—Ç–∏—Ç—å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –≤–º–µ—Å—Ç–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏.
- `--avgRuns`: –ó–∞–ø—É—Å—Ç–∏—Ç—å –±–µ–Ω—á–º–∞—Ä–∫ N —Ä–∞–∑ –∏ —É—Å—Ä–µ–¥–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –±–æ–ª—å—à–µ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏.
- `--verbose` –∏–ª–∏ `-v`: –í—ã–≤–æ–¥–∏—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–µ –ª–æ–≥–∏ –æ –∑–∞–≥—Ä—É–∑–∫–µ –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ engine.

**–ü—Ä–∏–º–µ—Ä:**

```bash
trtexec --loadEngine=<engine_file.plan> --warmUp=2000 --duration=10 --avgRuns=5
```

### perf_analyzer

–í—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `perf_analyzer` –∏–∑ Triton SDK –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫–∞–∫ –¥–ª—è `ONNX`, —Ç–∞–∫ –∏ –¥–ª—è `TensorRT`. –î–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å SDK:

```bash
docker run --rm -it --net host \
  nvcr.io/nvidia/tritonserver:24.02-py3-sdk
```

–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –≤–µ—Ä—Å–∏—è SDK ‚Äî `24.02`, –ø–æ—Å–∫–æ–ª—å–∫—É –≤ –≤–µ—Ä—Å–∏–∏ `25.06` –∏–º–µ–µ—Ç—Å—è [–ø—Ä–æ–±–ª–µ–º–∞ —Å OOM](https://github.com/triton-inference-server/perf_analyzer/issues/84)

–ó–∞—Ç–µ–º –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –∫–æ–º–∞–Ω–¥—É `perf_analyzer` —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏, –ø–æ–¥—Ö–æ–¥—è—â–∏–º–∏ –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ ASR:

```bash
perf_analyzer -u localhost:8001 \
  -i grpc \
  -a \
  -m streaming_acoustic \
  --streaming \
  --sequence-length=50 \
  --measurement-mode=count_windows \
  --measurement-request-count=5000 \
  --request-rate-range=2048:4096:256 \
  --stability-percentage=100 \
  --latency-threshold=100
```

**–ö–ª—é—á–µ–≤—ã–µ —Ñ–ª–∞–≥–∏:**

- `--sequence-length`: –î–ª–∏–Ω–∞ –≤—Ö–æ–¥–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π. sequence-length = 1 —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç 300 –º—Å –∞—É–¥–∏–æ
- `--request-rate-range`: –î–∏–∞–ø–∞–∑–æ–Ω —á–∞—Å—Ç–æ—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
- `--latency-threshold`: –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ø–æ—Ä–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∑–∞–¥–µ—Ä–∂–∫–∏ –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö

## –†–µ–∑—É–ª—å—Ç–∞—Ç—ã

–ü—Ä–∏–º–µ—Ä –≤—ã–≤–æ–¥–∞ `perf_analyzer` –¥–ª—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å `RTX 3090` –∏ `TensorRT`, —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ `16`:
```bash
Inferences/Second vs. Client Average Batch Latency
Request Rate: 2048.00, throughput: 2045.52 infer/sec, latency 13250 usec
Request Rate: 2304.00, throughput: 2300.63 infer/sec, latency 12819 usec
Request Rate: 2560.00, throughput: 2555.53 infer/sec, latency 13700 usec
Request Rate: 2816.00, throughput: 2804.02 infer/sec, latency 20469 usec
Request Rate: 3072.00, throughput: 2902.08 infer/sec, latency 41838 usec
Request Rate: 3328.00, throughput: 2887.90 infer/sec, latency 38171 usec
Request Rate: 3584.00, throughput: 2811.21 infer/sec, latency 32192 usec
Request Rate: 3840.00, throughput: 2788.82 infer/sec, latency 25986 usec
Request Rate: 4096.00, throughput: 2745.62 infer/sec, latency 27433 usec
```

–í—ã –º–æ–∂–µ—Ç–µ —Ç–∞–∫–∂–µ –≤—ã—á–∏—Å–ª–∏—Ç—å –ø—Ä–æ–ø—É—Å–∫–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ø–æ —Ñ–æ—Ä–º—É–ª–µ: `SPS = inferences/sec * chuck size (sec)`. –î–ª—è –ø—Ä–∏–≤–µ–¥—ë–Ω–Ω–æ–≥–æ –≤—ã—à–µ –ø—Ä–∏–º–µ—Ä–∞ —ç—Ç–æ –¥–∞—ë—Ç –ø—Ä–æ–ø—É—Å–∫–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å `3000 * 0.3 = 900 SPS`, –ø—Ä–∏ —ç—Ç–æ–º –∑–∞–¥–µ—Ä–∂–∫–∞ –Ω–∞ –æ–¥–∏–Ω —á–∞–Ω–∫ –æ—Å—Ç–∞—ë—Ç—Å—è –Ω–∏–∂–µ 100 –º—Å.
